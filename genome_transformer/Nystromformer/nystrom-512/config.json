{
  "model_checkpoints": "/model/model",
  "data_folder": "/dataset",
  "glue_dataset_folder": "/glue",
  "wikihop_dataset_folder": "/wikihop",
  "model": {
      "mixed_precision": true,
      "attention_grad_checkpointing": false,
      "gelu_grad_checkpointing": true,
      "vocab_size": 50265,
      "num_sen_type": 1,
      "max_seq_len": 512,
      "embedding_dim": 768,
      "transformer_dim": 768,
      "transformer_hidden_dim": 3072,
      "num_layers": 12,
      "dropout_prob": 0.1,
      "num_head": 12,
      "head_dim": 64,
      "attn_type": "nystrom",
      "num_landmarks": 512,
      "seq_len": 64,
      "conv_kernel_size": 33
  },
  "pretraining_setting": {
      "batch_size": 256,
      "learning_rate": 0.0001,
      "warmup": 0.01,
      "batches_per_report": 10,
      "batches_per_epoch": 5000,
      "epoch": 100,
      "validate_batches_per_epoch": 100
  },
  "gpu_setting": {
    "inst_per_gpu": 8
  }
}
